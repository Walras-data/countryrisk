{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Countryrisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is divided as follows:\n",
    "\n",
    "1 - Data Extraction and Data Wrangling\n",
    "2 - Creating Variables and Cleaning the Data\n",
    "3 - Setting up the Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing packages\n",
    "\n",
    "import wbdata\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import datetime\n",
    "import quandl\n",
    "%config IPCompleter.greedy=True\n",
    "import pandas_datareader\n",
    "import urllib3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Extracting and Cleaning the Data from IMF, WB and BIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Country selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fecthing country names from IMF database to get all ISO codes\n",
    "countrycodes = pd.read_csv('coucodes.csv', delimiter = ';', encoding = \"ISO-8859-1\")\n",
    "countrycodes = countrycodes[[\"IMF Name\", \"ISO Code\", \"ISO2 Code\"]]\n",
    "countrycodes.reset_index(level=0, inplace=True)\n",
    "countrycodes = countrycodes.rename(columns={'IMF Name': 'country'})\n",
    "\n",
    "countries = [\"Brazil\",  \"Mexico\", \"India\",  \"Russia\", \"Uruguay\",\n",
    "             \"Korea\", \"Thailand\", \"Costa Rica\", \"Colombia\", \"Paraguay\",\n",
    "             \"Chile\", \"South Africa\", \"Taiwan\", \"Turkey\", \"Ukraine\", \"Nigeria\", \"Indonesia\",\n",
    "             \"Bangladesh\", \"Philippines\", \"Pakistan\", \"Egypt\", \"Ethiopia\", \"Vietnam\", \n",
    "             \"Myanmar\", \"Algeria\", \"Sudan\", \"Uganda\", \"Morocco\",  \"Malaysia\",\n",
    "             \"Afghanistan\", \"Ghana\", \"Laos\", \"Singapore\", \"Poland\",\n",
    "             \"Israel\", \"Czech Republic\", \"Romania\", \"Hungary\", \"Kazakhstan\", \"Kenya\", \"Angola\",\n",
    "            \"Dominican Republic\", \"Sri Lanka\", \"Guatemala\", \"Bulgaria\", \"Tanzania\", \"Belaurus\",\n",
    "            \"Croatia\",  \"Democratic Republic of the Congo\",\n",
    "            \"Azerbaijan\", \"Côte d'Ivoire\", \"Argentina\", \"Iran\", \"Iraq\", \"Venezuela\"]\n",
    "\n",
    "countrycodes2 = countrycodes.copy()\n",
    "countrycodes2 = countrycodes2[[\"country\", \"ISO2 Code\"]]\n",
    "countrycodes2 = countrycodes2.dropna()\n",
    "\n",
    "countrycodes = countrycodes[countrycodes[\"country\"].isin(countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up date time structure so that python understands is a panel.\n",
    "\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "\n",
    "today = datetime.today()\n",
    "datem = datetime(today.year, today.month, 1)\n",
    "\n",
    "year = pd.date_range('1995-01-01', end = str(today.month-1) + \"-\" + str(today.year), freq='MS').strftime('%Y-%m')\n",
    "\n",
    "\n",
    "base = pd.DataFrame(data=list(product(year, countrycodes[\"ISO2 Code\"])), columns=['year','ISO2 Code'])\n",
    "base = base.sort_values(by=['ISO2 Code', \"year\"]).dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## World Bank data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading data from worldbank\n",
    "\n",
    "from pandas_datareader import wb\n",
    "\n",
    "worldbank_data = pandas_datareader.wb.download(indicator = [\"DSTKMKTXD\"], country = countrycodes[\"ISO Code\"], start = 2010, end = 2020, freq = \"M\")\n",
    "worldbank_data2 = pandas_datareader.wb.download(indicator =[\"DSTKMKTXD\"], country = countrycodes[\"ISO Code\"], start = 1999, end = 2009, freq = \"M\")\n",
    "worldbank_data3 = pandas_datareader.wb.download(indicator = [\"DSTKMKTXD\"], country = countrycodes[\"ISO Code\"], start = 1990, end = 1998, freq = \"M\")\n",
    "\n",
    "worldbank_data = pd.concat([worldbank_data, worldbank_data2])\n",
    "worldbank_data = pd.concat([worldbank_data, worldbank_data3])\n",
    "\n",
    "worldbank_data.reset_index(inplace=True)\n",
    "\n",
    "worldbank_data_conc = pd.merge(worldbank_data, countrycodes, on=\"country\")\n",
    "worldbank_data_conc[\"year\"] = pd.to_datetime(worldbank_data_conc[\"year\"], format='%YM%m')\n",
    "worldbank_data_conc.drop([\"country\", \"ISO Code\", \"index\"],1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are running R from inside python to get information from the IMF API. \n",
    "\n",
    "Reasons:\n",
    "\n",
    "i) IMF´S Python API cannot extract the whole sample of countries.\n",
    "ii) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run R script to import imf data - \n",
    "\n",
    "### For replication purposes please:\n",
    "\n",
    "### Install R\n",
    "\n",
    "### Change file path to your own hard drive.\n",
    "\n",
    "import subprocess\n",
    "retcode = subprocess.call(['C:/Program Files/R/R-3.5.1/bin/Rscript.exe', '--vanilla', \n",
    "                           'C:/Users/vitor/OneDrive/Documentos/databowl/teste.R'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stacking and renaming IMF columns.\n",
    "\n",
    "stack = pd.read_csv(\"teste2.csv\")\n",
    "stack =stack.rename(columns = {'x_gdp':'PCTOT', \"iso2c\": \"ISO2 Code\", \"year_month\": \"year\"})\n",
    "stack.drop(\"Unnamed: 0\", inplace = True, axis = 1)\n",
    "stack[\"year\"] = pd.to_datetime(stack[\"year\"], format='%Y-%m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merging IMF and WB dataframes.\n",
    "\n",
    "full = pd.merge(stack, worldbank_data_conc, left_on = [\"ISO2 Code\", \"year\"], right_on = [\"ISO2 Code\", \"year\"], how = \"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Couprisk\n",
    "\n",
    "Getting information from Couprisk database. \n",
    "\n",
    "Specifically, the indicator that we´re most interested is a variable that measures the risk of a regime change in the country.\n",
    "This being a proxy of general political volatility in the particular country.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calling bases from the Couprisk API\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = \"https://oefdatascience.github.io/REIGN.github.io/menu/reign_current.html\"\n",
    "\n",
    "page = requests.get(url)    \n",
    "data = page.text\n",
    "soup = BeautifulSoup(data)\n",
    "\n",
    "links = []\n",
    "for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "\n",
    "sub = \"https://cdn.rawgit.com/OEFDataScience/REIGN.github.io/gh-pages/data_sets/REIGN\"\n",
    "\n",
    "\n",
    "link = [word for word in links if word.startswith(sub)]\n",
    "\n",
    "import pandas as pd\n",
    "couprisk = pd.read_csv(link[0])\n",
    "couprisk = couprisk[[\"country\", \"year\", \"month\", \"couprisk\", \"ccode\"]]\n",
    "couprisk[\"month\"] = couprisk[\"month\"].astype(int)\n",
    "couprisk[\"month\"] = couprisk.month.map(\"{:02}\".format)\n",
    "couprisk[\"year\"] = couprisk[\"year\"].round().astype(int).astype(str) + \"-\" + couprisk[\"month\"].astype(str)+\"-\" + \"01\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "### data wrangling\n",
    "\n",
    "cowcode = pd.read_csv(\"coucodes2.csv\", sep = \";\")\n",
    "cowcode = cowcode.drop_duplicates()\n",
    "\n",
    "#couprisk = couprisk[couprisk[\"country\"].isin(countries)]\n",
    "couprisk = pd.merge(couprisk, cowcode, left_on = \"ccode\", right_on = \"cowcode2\")\n",
    "\n",
    "couprisk = couprisk.drop([\"country\", \"ccode\", \"cowcode2\"], 1)\n",
    "couprisk = couprisk.drop_duplicates([\"year\", \"ccdcodelet\"])\n",
    "couprisk = couprisk.rename(columns={'ccdcodelet': 'ISO Code'})\n",
    "couprisk = pd.merge(couprisk, countrycodes, on = \"ISO Code\")\n",
    "couprisk = couprisk.drop([\"index\", \"ISO Code\", \"country\"], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting election years information\n",
    "\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = \"https://oefdatascience.github.io/REIGN.github.io/menu/reign_current.html\"\n",
    "\n",
    "page = requests.get(url)    \n",
    "data = page.text\n",
    "soup = BeautifulSoup(data)\n",
    "\n",
    "links = []\n",
    "for link in soup.find_all(attrs={'href': re.compile(\"http\")}):\n",
    "    links.append(link.get('href'))\n",
    "\n",
    "\n",
    "sub = \"https://cdn.rawgit.com/OEFDataScience/REIGN.github.io/gh-pages/data_sets/electionlist\"\n",
    "\n",
    "\n",
    "link = [word for word in links if word.startswith(sub)]\n",
    "\n",
    "import pandas as pd\n",
    "election = pd.read_csv(link[0])\n",
    "election = election[[\"country\", \"elec_year\", \"ccode\", \"elec_month\"]]\n",
    "election[\"elec_month\"] = election[\"elec_month\"].fillna(1).astype(int)\n",
    "election[\"elec_month\"] = election.elec_month.map(\"{:02}\".format)\n",
    "election[\"elec_year\"] = election[\"elec_year\"].round().astype(int).astype(str) + \"-\" + election[\"elec_month\"].astype(str)+\"-\" + \"01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data wrangling\n",
    "\n",
    "election = pd.merge(election, cowcode, left_on = \"ccode\", right_on = \"cowcode2\")\n",
    "\n",
    "election = election.drop([\"country\", \"ccode\", \"cowcode2\"], 1)\n",
    "#election = election.drop_duplicates([\"year\", \"ccdcodelet\"])\n",
    "election = election.rename(columns={'ccdcodelet': 'ISO Code', \"elec_year\" : \"year\"})\n",
    "election = pd.merge(election, countrycodes, on = \"ISO Code\")\n",
    "election = election.drop([\"index\", \"ISO Code\", \"country\", \"elec_month\"], 1)\n",
    "election[\"election\"] = 1\n",
    "election = election.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merging dataframe with couprisk and election year indicators\n",
    "\n",
    "couprisk[\"year\"] = pd.to_datetime(couprisk[\"year\"], errors = \"coerce\")\n",
    "election[\"year\"] = pd.to_datetime(election[\"year\"], errors = \"coerce\")\n",
    "\n",
    "\n",
    "full = pd.merge(full, couprisk, on = [\"ISO2 Code\", \"year\"], how = \"left\")\n",
    "full = pd.merge(full, election, on = [\"ISO2 Code\", \"year\"], how = \"left\")\n",
    "\n",
    "full[\"election\"] = full[\"election\"].fillna(0)\n",
    "full = full[full[\"ISO2 Code\"] != \"RO\"]\n",
    "full = full[full[\"ISO2 Code\"] != \"BH\"]\n",
    "full = full[full.year.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting information from the IMF´s World Economic Outlook \n",
    "\n",
    "GGXWDG_NGDP = pd.read_csv(\"GGXWDG_NGDP.csv\", error_bad_lines=False, sep = \";\")\n",
    "GGXWDG_NGDP = pd.melt(GGXWDG_NGDP, id_vars=['country'])\n",
    "GGXWDG_NGDP = pd.merge(GGXWDG_NGDP, countrycodes2, on = \"country\", how = \"left\")\n",
    "GGXWDG_NGDP = GGXWDG_NGDP.dropna()\n",
    "GGXWDG_NGDP[\"variable\"] = pd.to_datetime(GGXWDG_NGDP[\"variable\"], errors = \"coerce\")\n",
    "GGXWDG_NGDP = GGXWDG_NGDP.rename(columns = {\"value\": \"GGXWDG_NGDP\", \"variable\" : \"year\"})\n",
    "\n",
    "GGXCNL_NGDP = pd.read_csv(\"GGXCNL_NGDP.csv\", error_bad_lines=False, sep = \";\")\n",
    "GGXCNL_NGDP = pd.melt(GGXCNL_NGDP, id_vars=['country'])\n",
    "GGXCNL_NGDP = pd.merge(GGXCNL_NGDP, countrycodes2, on = \"country\", how = \"left\")\n",
    "GGXCNL_NGDP = GGXCNL_NGDP.dropna()\n",
    "GGXCNL_NGDP[\"variable\"] = pd.to_datetime(GGXCNL_NGDP[\"variable\"], errors = \"coerce\")\n",
    "GGXCNL_NGDP = GGXCNL_NGDP.rename(columns = {\"value\": \"GGXCNL_NGDP\", \"variable\" : \"year\"})\n",
    "\n",
    "full = pd.merge(full, GGXWDG_NGDP, on = [\"ISO2 Code\", \"year\"], how = \"left\")\n",
    "full =pd.merge(full,  GGXCNL_NGDP, on = [\"ISO2 Code\", \"year\"], how = \"left\")\n",
    "\n",
    "full = full.drop([\"country_x\", \"country_y\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting oil price information from Quandl API.\n",
    "\n",
    "import quandl\n",
    "quandl.ApiConfig.api_key = \"e2KZ8SFfstro3DH2_zvj\"\n",
    "\n",
    "oil = quandl.get(\"ODA/POILWTI_USD\")\n",
    "oil.index = oil.index + pd.offsets.MonthBegin(0)\n",
    "oil.reset_index(inplace = True)\n",
    "oil = oil.rename(columns = {\"Date\": \"year\", \"Value\":\"oil\"})\n",
    "full = pd.merge(full, oil, on = \"year\", how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting VIX information from yahoo finance API.\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "msft = yf.Ticker(\"^VIX\")\n",
    "vix = msft.history(period=\"max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging VIX information with dataframe\n",
    "\n",
    "vix.sort_index(inplace=True)\n",
    "vix2 = vix.copy()\n",
    "vix2.index = pd.to_datetime(vix2.index) + pd.offsets.MonthBegin(0)\n",
    "vix2 = vix2[\"Open\"].to_frame()\n",
    "vix2 = vix2.loc[~vix2.index.duplicated(keep='first')]\n",
    "vix2.reset_index(inplace = True)\n",
    "vix2 = vix2.rename(columns = {\"Date\": \"year\", \"Open\":\"vix\"})\n",
    "\n",
    "full = pd.merge(full, vix2, on = \"year\", how = \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Subsetting dataframe to select just the chosen countries sample.\n",
    "\n",
    "full = full[full[\"ISO2 Code\"].isin(countrycodes[\"ISO2 Code\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Creating variables and Cleaning the Data\n",
    "\n",
    "The following section is devoted to cleaning the dataframe as well as creating variables that are relevant to the modelling strategy that is going to be used in the Machine Learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming columns to numeric variables\n",
    "\n",
    "cols = full.columns.drop([\"ISO2 Code\", \"year\"])\n",
    "\n",
    "full[cols] = full[cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to numeric\n",
    "cols = full.columns.drop([\"ISO2 Code\", \"year\"])\n",
    "\n",
    "full[cols] = full[cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating indicator variables\n",
    "\n",
    "\n",
    "full[\"broad_reserves_ratio\"] = full[\"FDSB_XDC\"]/(full[\"RAXG_USD\"]*full[\"ENDE_XDC_USD_RATE\"]).diff(1)\n",
    "full[\"broad_base_ratio\"] = full[\"FDSB_XDC\"]/full[\"FASMB_XDC\"].diff(1)\n",
    "full[\"foreignassets_reserves_ratio\"] = full[\"FDSF_XDC\"]/(full[\"RAXG_USD\"]*full[\"ENDE_XDC_USD_RATE\"]).diff(1)\n",
    "\n",
    "full[\"inflation\"] = full.groupby('ISO2 Code', sort=False).PCPI_IX.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"BCAXF_BP6_USD\"] = full[\"BCAXF_BP6_USD\"]*full[\"ENDE_XDC_USD_RATE\"]/full[\"NGDP_XDC\"]\n",
    "\n",
    "\n",
    "full[\"current_account\"] = full.groupby('ISO2 Code', sort=False).BCAXF_BP6_USD.apply(\n",
    "     lambda x: x.pct_change(12)) \n",
    "\n",
    "full[\"moneymarket_rate\"] = full.groupby('ISO2 Code', sort=False).FIMM_PA.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"reserves\"] = full[\"RAXG_USD\"].diff(1)\n",
    "full[\"reserves_12p\"] = full.groupby('ISO2 Code', sort=False).RAXG_USD.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"stocks\"] = full.groupby('ISO2 Code', sort=False).DSTKMKTXD.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"commodities_index\"] =  full.groupby('ISO2 Code', sort=False).PCTOT.apply(\n",
    "     lambda x: x.pct_change(12))  \n",
    "\n",
    "full[\"exports\"] = full.groupby('ISO2 Code', sort=False).TXG_FOB_USD.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"imports\"] = full.groupby('ISO2 Code', sort=False).TMG_CIF_USD.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"imports_fx\"] = (full[\"TMG_CIF_USD\"]/full[\"RAXG_USD\"]).diff(1)\n",
    "\n",
    "full[\"exchange_change\"] = full.groupby('ISO2 Code', sort=False).ENDE_XDC_USD_RATE.apply(\n",
    "     lambda x: x.pct_change(1)).shift(1)\n",
    "\n",
    "full[\"exchange_change_12m\"] = full.groupby('ISO2 Code', sort=False).ENDE_XDC_USD_RATE.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"net_debt\"] = full.groupby('ISO2 Code', sort=False).GGXWDG_NGDP.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"lending_borrowing\"] = full.groupby('ISO2 Code', sort=False).GGXCNL_NGDP.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"lending_rate\"] = full.groupby('ISO2 Code', sort=False).FILR_PA.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"M2\"] = full.groupby('ISO2 Code', sort=False).FM2_XDC.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"claims_private\"] = full.groupby('ISO2 Code', sort=False).FDSAOP_XDC.apply(\n",
    "     lambda x: x.pct_change(12)).diff(1)\n",
    "\n",
    "full[\"vix\"] = full.groupby('ISO2 Code', sort=False).vix.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"oil\"] = full.groupby('ISO2 Code', sort=False).oil.apply(\n",
    "     lambda x: x.pct_change(12))\n",
    "\n",
    "full[\"lag_1\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(-1).fillna(0)\n",
    "full[\"lag_2\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(-2).fillna(0)\n",
    "full[\"lag_3\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(-3).fillna(0)\n",
    "full[\"lag_4\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(-4).fillna(0)\n",
    "full[\"lag_5\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(-5).fillna(0)\n",
    "full[\"lag_6\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(-6).fillna(0)\n",
    "full['election_p'] =  full[\"lag_1\"] + full[\"lag_2\"] + full[\"lag_3\"] + full[\"lag_4\"] + full[\"lag_5\"] + full[\"lag_6\"]\n",
    "full['election_p'] = np.where(full['election_p'] > 0, 1, 0)\n",
    "\n",
    "full[\"lag_1\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(1).fillna(0)\n",
    "full[\"lag_2\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(2).fillna(0)\n",
    "full[\"lag_3\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(3).fillna(0)\n",
    "full[\"lag_4\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(4).fillna(0)\n",
    "full[\"lag_5\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(5).fillna(0)\n",
    "full[\"lag_6\"] =full.groupby(\"ISO2 Code\")[\"election\"].shift(6).fillna(0)\n",
    "full['election'] =  full[\"lag_1\"] + full[\"lag_2\"] + full[\"lag_3\"] + full[\"lag_4\"] + full[\"lag_5\"] + full[\"lag_6\"]\n",
    "full['election'] = np.where(full['election'] > 0, 1, 0)\n",
    "\n",
    "\n",
    "### Dropping variables that are not going to be used\n",
    "    \n",
    "full.drop([\"FASMB_XDC\", \"FDSB_XDC\", \"FDSF_XDC\", \"FIMM_PA\",  \"PCPI_IX\", \"RAXG_USD\", \"PCTOT\",\n",
    "           \"BCAXF_BP6_USD\", \"DSTKMKTXD\", \"FILR_PA\", \"GGXCNL_NGDP\", \"GGXWDG_NGDP\", \"TXG_FOB_USD\", \"TMG_CIF_USD\",\n",
    "          \"LUR_PT\", \"FDSAOP_XDC\", \"AIP_IX\", \"EREER_IX\", \"BGS_BP6_USD\", \"NGDP_XDC\", \"FM2_XDC\", \"RAXGFX_USD\",\n",
    "          \"FMB_XDC\", \"FISR_PA\", \"lag_1\", \"lag_2\", \"lag_3\", \"lag_4\", 'lag_5', \"lag_6\"],1, inplace = True)                                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replacing inf and -inf with nans \n",
    "\n",
    "full.set_index([\"year\", \"ISO2 Code\"], inplace = True)\n",
    "\n",
    "#full = full.groupby(level=1).ffill()\n",
    "full = full.replace([np.inf, -np.inf], np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replacing nans with zero \n",
    "\n",
    "full.exchange_change = full.exchange_change.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns used to create the new variables\n",
    "#optional code to create country dummies\n",
    "\n",
    "#full[\"ISO\"] = full.index.get_level_values(1)\n",
    "#full = pd.get_dummies(full, columns=['ISO'], prefix = ['Country'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of dependent variable\n",
    "\n",
    "The following chunks are devoted to creating the exchange rate crisis variable - our variable of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "full_fill = full\n",
    "#FIRST DEFINITION\n",
    "full_fill[\"dummy\"] = np.where(full_fill['exchange_change']  >0.20, 1, 0) #10 percent variation\n",
    "\n",
    "#SECOND DEFINITION\n",
    "#full_fill[\"threshold\"] = full_fill.groupby(\"ISO2 Code\")[\"exchange_change\"].transform(\"mean\") + (full_fill.groupby(\"ISO2 Code\")[\"exchange_change\"].transform(\"std\"))\n",
    "#full_fill[\"dummy\"] = np.where(full_fill['exchange_change'] > full_fill[\"threshold\"], 1, 0) # > 2 stdev threshold\n",
    "\n",
    "#THIRD DEFINITION\n",
    "#full_fill[\"dummy\"] = np.where(full_fill['exchange_change_12m'].shift(-6) >0.2, 1, 0) #50 percent variation in the next 12 months\n",
    "\n",
    "crise = full_fill\n",
    "\n",
    "crise[\"lag_1\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-1).fillna(0)\n",
    "crise[\"lag_2\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-2).fillna(0)\n",
    "crise[\"lag_3\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-3).fillna(0)\n",
    "crise[\"lag_4\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-4).fillna(0)\n",
    "crise[\"lag_5\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-5).fillna(0)\n",
    "crise[\"lag_6\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-6).fillna(0)\n",
    "crise[\"lag_7\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-7).fillna(0)\n",
    "crise[\"lag_8\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-8).fillna(0)\n",
    "crise[\"lag_9\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-9).fillna(0)\n",
    "crise[\"lag_10\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-10).fillna(0)\n",
    "crise[\"lag_11\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-11).fillna(0)\n",
    "crise[\"lag_12\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-12).fillna(0)\n",
    "crise[\"lag_13\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-13).fillna(0)\n",
    "crise[\"lag_14\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-14).fillna(0)\n",
    "crise[\"lag_15\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-15).fillna(0)\n",
    "crise[\"lag_16\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-16).fillna(0)\n",
    "crise[\"lag_17\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-17).fillna(0)\n",
    "crise[\"lag_18\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-18).fillna(0)\n",
    "crise[\"lag_19\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-19).fillna(0)\n",
    "crise[\"lag_20\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-20).fillna(0)\n",
    "crise[\"lag_21\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-21).fillna(0)\n",
    "crise[\"lag_22\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-22).fillna(0)\n",
    "crise[\"lag_23\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-23).fillna(0)\n",
    "crise[\"lag_24\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-24).fillna(0)\n",
    "#crise[\"yt-1\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(0)\n",
    "#crise[\"yt2-1\"] = crise.groupby(\"ISO2 Code\")[\"exchange_change\"].shift(1)\n",
    "#crise[\"yt2-2\"] = crise.groupby(\"ISO2 Code\")[\"exchange_change\"].shift(2)\n",
    "#crise[\"yt2-3\"] = crise.groupby(\"ISO2 Code\")[\"exchange_change\"].shift(3)\n",
    "\n",
    "crise[\"dummy\"].fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "crise['indicator'] =  crise[\"lag_1\"] + crise[\"lag_2\"] + crise[\"lag_3\"] + crise[\"lag_4\"] + crise[\"lag_5\"] + crise[\"lag_6\"] + crise[\"lag_7\"] + crise[\"lag_8\"] + crise[\"lag_9\"] + crise[\"lag_10\"] + crise[\"lag_11\"] +crise[\"lag_12\"]\n",
    "crise['indicator2'] = np.where(crise['indicator'] > 0, 1, 0)\n",
    "\n",
    "#crise['indicator2'] = crise[\"dummy\"] #Third Definition\n",
    "\n",
    "#bs2[\"lag_1\"] +\n",
    "#crise['indicator'] = crise[\"lag_1\"] +crise[\"lag_2\"]  + crise[\"lag_3\"] + crise[\"lag_4\"] + crise[\"lag_5\"] +crise[\"lag_6\"]\n",
    "#crise['indicator2'] = np.where(crise['indicator'] > 0, 1, 0)\n",
    "\n",
    "#crise['indicator'] =  crise[\"lag_1\"] + crise[\"lag_2\"] + crise[\"lag_3\"] + crise[\"lag_4\"] + crise[\"lag_5\"] + crise[\"lag_6\"] + crise[\"lag_7\"] + crise[\"lag_8\"] + crise[\"lag_9\"] + crise[\"lag_10\"] + crise[\"lag_11\"] +crise[\"lag_12\"] + crise[\"lag_13\"] + crise[\"lag_14\"] + crise[\"lag_15\"] + crise[\"lag_16\"] + crise[\"lag_17\"] + crise[\"lag_18\"] + crise[\"lag_19\"] + crise[\"lag_20\"] + crise[\"lag_21\"] + crise[\"lag_22\"] + crise[\"lag_23\"] +crise[\"lag_24\"]\n",
    "#crise['indicator2'] = np.where(crise['indicator'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining final dataframe to be used in the ML pipeline\n",
    "\n",
    "full_fill = crise\n",
    "full_fill.reset_index(inplace = True)\n",
    "full_fill.set_index(\"year\", inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Machine Learning Pipeline\n",
    "\n",
    "This section is devoted to creating the Machine Learning pipeline for currency crisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calling all necessary packages.\n",
    "\n",
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from xgboost import XGBClassifier \n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping irrelevant collumns\n",
    "\n",
    "full_fill.set_index(\"ISO2 Code\", append = True, inplace = True)\n",
    "columns = full_fill.columns\n",
    "columns = columns.drop([\"lag_1\", \"lag_2\", \"lag_3\", \"lag_4\", \"lag_5\", \"lag_6\", \"lag_7\", \"lag_8\", \"lag_9\", \"lag_10\", \"lag_11\", \"lag_12\", \"lag_13\", \"lag_14\", \"lag_15\", \"lag_16\", \"lag_17\", \"lag_18\", \"lag_19\", \"lag_20\", \"lag_21\", \"lag_22\", \"lag_23\", \"lag_24\", \"indicator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Optional section: creating lag variables.\n",
    "\n",
    "\n",
    "full_fill2 = full_fill.query('year > \"1990-12-01\"')\n",
    "#full_fill3 = full_fill2\n",
    "#full_fill2 =  full_fill2[full_fill2.columns.drop(list(full_fill2.filter(regex='Country')))]\n",
    "columns2 = columns.drop(list(full_fill.filter(regex='Country')))\n",
    "#columns2 = [\"inflation\", \"reserves_gdp\", \"commodities_growth\", \"couprisk\", \"ISO2 Code\"]\n",
    "X =  full_fill2[columns2]#.groupby(level=1).diff(1)\n",
    "#X = X.drop([\"ISO2 Code\"],1)\n",
    "X_lag = full_fill2[columns2].groupby(level=1).shift(1)\n",
    "X_lag2 = full_fill2[columns2].groupby(level=1).shift(2)\n",
    "X_lag3 = full_fill2[columns2].groupby(level=1).shift(3)\n",
    "X_lag4 = full_fill2[columns2].groupby(level=1).diff(12).shift(4)\n",
    "X_lag5 = full_fill2[columns2].groupby(level=1).diff(12).shift(5)\n",
    "\n",
    "X_level = full_fill2[columns2]\n",
    "\n",
    "X_lag = X_lag.add_suffix('_lag')\n",
    "X_lag2 = X_lag2.add_suffix('_lag2')\n",
    "X_lag3 = X_lag3.add_suffix('_lag3')\n",
    "X_lag4 = X_lag4.add_suffix('_lag4')\n",
    "X_lag5 = X_lag5.add_suffix('_lag5')\n",
    "\n",
    "X_level = X_level.add_suffix('_level')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y = full_fill2['indicator2']\n",
    "#X = pd.concat((X, full_fill3.filter(regex='Country')), axis=1)\n",
    "#X = pd.concat([X, X_lag], axis=1)\n",
    "#X = pd.concat([X, X_lag2], axis=1)\n",
    "#X = pd.concat([X, X_lag3], axis=1)\n",
    "#X = pd.concat([X, X_lag4], axis=1)\n",
    "#X = pd.concat([X, X_lag5], axis=1)\n",
    "#X = pd.concat([X, X_level], axis=1)\n",
    "\n",
    "#X = X.fillna(X.mean())\n",
    "\n",
    "#X = X.groupby('ISO2 Code_level').transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#scaler.fit(X)\n",
    "#X = pd.DataFrame(scaler.transform(X), columns=X.columns, index = X.index)\n",
    "\n",
    "X = X.sort_index(level = 0) #For CV purpose\n",
    "#X = X.drop([\"ENDE_XDC_USD_RATE\"], 1)\n",
    "Y = Y.sort_index(level = 0) #For CV purpose\n",
    "\n",
    "#X = X.fillna(X.mean())\n",
    "\n",
    "X_final = X.drop([\"indicator2\", \"month\"],1)\n",
    "#cols_to_shift = X_final.columns.drop(\"dummy\")\n",
    "X_final = X_final.groupby(level = 1).shift()\n",
    "\n",
    "\n",
    "X_train = X_final.query('year < \"2013-12-31\"')\n",
    "X_test = X_final.query('year > \"2013-12-31\"')\n",
    "\n",
    "Y_train = Y.to_frame().query('year < \"2013-12-31\"').iloc[:,0]\n",
    "Y_test = Y.to_frame().query('year > \"2013-12-31\"').iloc[:,0]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   12.2s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   26.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   40.8s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed: 10.9min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed: 12.9min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed: 15.3min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed: 18.0min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed: 21.7min\n",
      "[Parallel(n_jobs=-1)]: Done 405 tasks      | elapsed: 22.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 23.4min\n"
     ]
    }
   ],
   "source": [
    "### Running the Machine Learning Model with time-series validation\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "#cv = GapWalkForward(n_splits=10, gap_size=6, test_size=48)\n",
    "\n",
    "#sort by before\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "        'max_depth': [3,5,6, 10],\n",
    "        'n_estimators': [50, 100, 200, 500, 1000],\n",
    "        'learning_rate': [0.01, 0.001],\n",
    "        \"min_child_weight\" : [ 3, 5, 7 ],\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "#clf1 = GridSearchCV(XGBClassifier(), params, n_jobs = -1, \n",
    "                   #cv=cv, scoring='balanced_accuracy',verbose=2, refit=True)\n",
    "\n",
    "clf1 = RandomizedSearchCV(XGBClassifier(), params, random_state=1984, n_iter=100, cv=cv, verbose=10, n_jobs=-1, scoring = 'balanced_accuracy')\n",
    "\n",
    "\n",
    "#clf1 =   XGBClassifier(n_estimators = 50,  eval_metric=\"auc\", max_depth = 2)\n",
    "#clf1 = LogisticRegression(\"l2\", C = 0.1)\n",
    "#clf1 = KNeighborsClassifier(50)\n",
    "#clf1 = MLPClassifier()\n",
    "#clf1 = ExtraTreesClassifier()\n",
    "#clf.fit(X_train, Y_train)\n",
    "clf1.fit(X_train, Y_train)\n",
    "y_pred = clf1.predict(X_test)\n",
    "y_prob = clf1.predict_proba(X_test)[:,1]\n",
    "confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3062,   15],\n",
       "       [ 423,    0]], dtype=int64)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Showing the results of the confusion matrix.\n",
    "\n",
    "y_pred = clf1.predict(X_test)\n",
    "y_prob = clf1.predict_proba(X_test)[:,1]\n",
    "y_pred =  (y_prob >= 0.5).astype('int')\n",
    "confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitor\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\vitor\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "C:\\Users\\vitor\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Plotting the risk probability charts based on the model results\n",
    "\n",
    "lista = full_fill2.index.get_level_values(1).unique()\n",
    "\n",
    " \n",
    "y_pred = clf1.predict(X_final)\n",
    "y_prob = clf1.predict_proba(X_final)[:,1]\n",
    "y_pred =  (y_prob >= 0.2).astype('int')\n",
    "\n",
    "#confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "X3 = X_final.copy()\n",
    "X3[\"threshold\"] = 0.5\n",
    "\n",
    "X3[\"pred\"] = y_prob\n",
    "X3[\"precrisis\"] = Y\n",
    "\n",
    "\n",
    "\n",
    "X3.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "X3.set_index(\"year\")\n",
    "\n",
    "for iso in lista:\n",
    "\n",
    "    X2 = X3[X3[\"ISO2 Code\"] == iso]\n",
    "    \n",
    "    X2[\"ENDE_XDC_USD_RATE\"] = X2[\"ENDE_XDC_USD_RATE\"].ffill()\n",
    "    X2[\"ENDE_XDC_USD_RATE\"] = X2[\"ENDE_XDC_USD_RATE\"].apply(pd.to_numeric, errors='coerce')\n",
    "    X2[\"ENDE_XDC_USD_RATE\"] = (X2[\"ENDE_XDC_USD_RATE\"])/np.nanmax(X2[\"ENDE_XDC_USD_RATE\"])\n",
    "    #cycle, trend = sm.tsa.filters.hpfilter(X2[\"pred\"], lamb=5)\n",
    "    #X2[\"hp\"] = trend\n",
    "    #X2.plot(y= [\"dummy\",\"pred\", \"ENDE_XDC_USD_RATE\"], linewidth=5, figsize=(15,15))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    X2.reset_index(inplace = True)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "\n",
    "\n",
    "  #  fig.add_trace(go.Bar(x=X2[\"year\"],\n",
    "   #                 y=X2[\"dummy\"],\n",
    "    #                name='Crisis',\n",
    "     #               marker_color='rgb(264, 45, 45)',\n",
    "      #              width = 2678400000\n",
    "       #                ))\n",
    "        \n",
    "\n",
    "    fig.add_trace(go.Bar(x=X2[\"year\"],\n",
    "                    y=X2[\"precrisis\"],\n",
    "                    name='Precrisis',\n",
    "                    marker_color='rgb(120, 120, 120)',\n",
    "                    marker_line_color = 'rgb(128, 128, 128)',\n",
    "                    opacity = 0.4,\n",
    "                    width = 2678400000\n",
    "                        ))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "                    x=X2[\"year\"],\n",
    "                    y=X2['pred'],\n",
    "                    name=\"Prob\",\n",
    "                    line_color='deepskyblue',\n",
    "                    opacity=1,\n",
    "                    line=dict(color='deepskyblue', width=4\n",
    "                                  )))\n",
    "    fig.add_trace(go.Scatter(\n",
    "                    x=X2[\"year\"],\n",
    "                    y=X2['threshold'],\n",
    "                    name=\"threshold\",\n",
    "                    line_color='rgb(264, 45, 45)',\n",
    "                    opacity=1,\n",
    "                    line=dict(color='deepskyblue', width=4\n",
    "                                  )))\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "                    x=X2[\"year\"],\n",
    "                    y=X2[\"ENDE_XDC_USD_RATE\"],\n",
    "                    name=\"Exchange Rate\",\n",
    "                    line_color='dimgray',\n",
    "                    opacity=0.6,\n",
    "                    line=dict(color='firebrick', width=4)))\n",
    "\n",
    "\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=1000,\n",
    "        height=600)\n",
    "\n",
    "\n",
    "    fig.write_image(iso + \"_graph_xgb.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Undersampling test\n",
    "\n",
    "full_fill2 = full_fill.query('year > \"1990-12-01\"')\n",
    "\n",
    "#full_fill3 = full_fill2\n",
    "#full_fill2 =  full_fill2[full_fill2.columns.drop(list(full_fill2.filter(regex='Country')))]\n",
    "columns2 = columns.drop(list(full_fill.filter(regex='Country')))\n",
    "#columns2 = [\"inflation\", \"reserves_gdp\", \"commodities_growth\", \"couprisk\", \"ISO2 Code\"]\n",
    "X =  full_fill2[columns2]#.groupby(level=1).diff(1)\n",
    "#X = X.drop([\"ISO2 Code\"],1)\n",
    "X_lag = full_fill2[columns2].groupby(level=1).shift(1)\n",
    "X_lag2 = full_fill2[columns2].groupby(level=1).shift(2)\n",
    "X_lag3 = full_fill2[columns2].groupby(level=1).shift(3)\n",
    "X_lag4 = full_fill2[columns2].groupby(level=1).diff(12).shift(4)\n",
    "X_lag5 = full_fill2[columns2].groupby(level=1).diff(12).shift(5)\n",
    "\n",
    "X_level = full_fill2[columns2]\n",
    "\n",
    "X_lag = X_lag.add_suffix('_lag')\n",
    "X_lag2 = X_lag2.add_suffix('_lag2')\n",
    "X_lag3 = X_lag3.add_suffix('_lag3')\n",
    "X_lag4 = X_lag4.add_suffix('_lag4')\n",
    "X_lag5 = X_lag5.add_suffix('_lag5')\n",
    "\n",
    "X_level = X_level.add_suffix('_level')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y = full_fill2['indicator2']\n",
    "#X = pd.concat((X, full_fill3.filter(regex='Country')), axis=1)\n",
    "#X = pd.concat([X, X_lag], axis=1)\n",
    "#X = pd.concat([X, X_lag2], axis=1)\n",
    "#X = pd.concat([X, X_lag3], axis=1)\n",
    "#X = pd.concat([X, X_lag4], axis=1)\n",
    "#X = pd.concat([X, X_lag5], axis=1)\n",
    "#X = pd.concat([X, X_level], axis=1)\n",
    "\n",
    "#X = X.fillna(X.mean())\n",
    "\n",
    "#X = X.groupby('ISO2 Code_level').transform(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#scaler.fit(X)\n",
    "#X = pd.DataFrame(scaler.transform(X), columns=X.columns, index = X.index)\n",
    "\n",
    "X = X.sort_index(level = 0) #For CV purpose\n",
    "#X = X.drop([\"ENDE_XDC_USD_RATE\"], 1)\n",
    "Y = Y.sort_index(level = 0) #For CV purpose\n",
    "\n",
    "#X = X.fillna(X.mean())\n",
    "\n",
    "X_final = X.drop([\"indicator2\", \"month\"],1)\n",
    "#cols_to_shift = X_final.columns.drop(\"dummy\")\n",
    "X_final = X_final.groupby(level = 1).shift()\n",
    "\n",
    "\n",
    "X_train = X_final.query('year < \"2009-12-31\"')\n",
    "X_test = X_final.query('year > \"2009-12-31\"')\n",
    "\n",
    "Y_train = Y.to_frame().query('year < \"2009-12-31\"').iloc[:,0]\n",
    "Y_test = Y.to_frame().query('year > \"2009-12-31\"').iloc[:,0]\n",
    "\n",
    "\n",
    "train = pd.merge(X_train, Y_train, left_index=True, right_index=True)\n",
    "count_class_0, count_class_1 = train.indicator2.value_counts()\n",
    "\n",
    "df_class_0 = train[train['indicator2'] == 0]\n",
    "df_class_1 = train[train['indicator2'] == 1]\n",
    "\n",
    "#count_class_0, count_class_1 = train..value_counts()\n",
    "\n",
    "df_class_0_under = df_class_0.sample(count_class_1)\n",
    "df_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n",
    "\n",
    "X_train = df_test_under.drop(\"indicator2\", 1)\n",
    "Y_train = df_test_under[\"indicator2\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vitor\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning:\n",
      "\n",
      "The total space of parameters 60 is smaller than n_iter=100. Running 60 iterations. For exhaustive searches, use GridSearchCV.\n",
      "\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 60 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    7.9s\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:   16.9s\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:   24.7s\n",
      "[Parallel(n_jobs=-1)]: Done  97 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done 129 tasks      | elapsed:   37.8s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   43.2s\n",
      "[Parallel(n_jobs=-1)]: Done 165 tasks      | elapsed:   49.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   55.4s\n",
      "[Parallel(n_jobs=-1)]: Done 205 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 226 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 249 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 272 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 297 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 322 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 405 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 465 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 496 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 529 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 562 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5497,  105],\n",
       "       [ 235,   63]], dtype=int64)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "#cv = GapWalkForward(n_splits=10, gap_size=6, test_size=48)\n",
    "\n",
    "#sort by before\n",
    "cv = TimeSeriesSplit(n_splits=10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "        'max_depth': [3,5,6, 10, 15],\n",
    "        'n_estimators': [100, 200, 500, 1000],\n",
    "        \"min_child_weight\" : [ 3, 5, 7 ],\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "#clf1 = GridSearchCV(XGBClassifier(), params, n_jobs = -1, \n",
    "                   #cv=cv, scoring='balanced_accuracy',verbose=2, refit=True)\n",
    "\n",
    "clf2 = RandomizedSearchCV(XGBClassifier(), params, random_state=1984, n_iter=100, cv=cv, verbose=10, n_jobs=-1, scoring = 'balanced_accuracy')\n",
    "\n",
    "\n",
    "#clf2 =   XGBClassifier(n_estimators = 100,  eval_metric=\"auc\", max_depth = 10, min_child_weight = 3, num_leaves = 100)\n",
    "#clf2 = LogisticRegression()\n",
    "#clf1 = KNeighborsClassifier(50)\n",
    "#clf2 = MLPClassifier()\n",
    "#clf1 = ExtraTreesClassifier()\n",
    "#clf.fit(X_train, Y_train)\n",
    "clf2.fit(X_train, Y_train)\n",
    "y_pred = clf2.predict(X_test)\n",
    "y_prob = clf2.predict_proba(X_test)[:,1]\n",
    "confusion_matrix(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
