{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StringIO' from 'pandas.compat' (C:\\Users\\leomf\\Anaconda3\\lib\\site-packages\\pandas\\compat\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1b4ae6957a53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mquandl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'config'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IPCompleter.greedy=True'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_datareader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murllib3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas_datareader\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m from .data import (DataReader, Options, get_components_yahoo,\n\u001b[0m\u001b[0;32m      3\u001b[0m                    \u001b[0mget_dailysummary_iex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_data_enigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_data_famafrench\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                    \u001b[0mget_data_fred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_data_google\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_data_moex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                    \u001b[0mget_data_morningstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_data_quandl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_data_stooq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas_datareader\\data.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_datareader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforex\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAVForexReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_datareader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquotes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAVQuotesReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_datareader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msector\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAVSectorPerformanceReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas_datareader\\av\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_datareader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_BaseReader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas_datareader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRemoteDataError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas_datareader\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murlencode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes_to_str\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m from pandas_datareader._utils import (RemoteDataError, SymbolWarning,\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'StringIO' from 'pandas.compat' (C:\\Users\\leomf\\Anaconda3\\lib\\site-packages\\pandas\\compat\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import wbdata\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import datetime\n",
    "import quandl\n",
    "%config IPCompleter.greedy=True\n",
    "import pandas_datareader\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fecthing country names from IMF database to get all ISO codes\n",
    "countrycodes = pd.read_csv('coucodes.csv', delimiter = ';', encoding = \"ISO-8859-1\")\n",
    "Eastern = countrycodes[\"Region (IMF)\"] == \"Eastern Europe\"\n",
    "Western = countrycodes[\"Region (IMF)\"] == \"Western Europe\"\n",
    "countrycodes = countrycodes[Eastern | Western]\n",
    "countrycodes = countrycodes[[\"IMF Name\", \"ISO Code\", \"ISO2 Code\"]]\n",
    "countrycodes.reset_index(level=0, inplace=True)\n",
    "countrycodes = countrycodes.rename(columns={'IMF Name': 'country'})\n",
    "\n",
    "ECB_countries = [\"Austria\", \"Belgium\", \"Denmark\", \"Finland\", \"France\", \"Germany\", \"Greece\", \"Ireland\",\"Italy\", \"Luxembourg\", \"Netherlands\", \"Portugal\", \"Spain\", \"Sweden\", \"United Kingdom\"]\n",
    "countrycodes = countrycodes[countrycodes[\"country\"].isin(ECB_countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: loading IMF data into pandas\n",
    "#Monthly data\n",
    "# Import libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "url = \"http://dataservices.imf.org/REST/SDMX_JSON.svc/CompactData/IFS/Q.\"\n",
    "\n",
    "#Countries\n",
    "couvalues = list(countrycodes[\"ISO2 Code\"].values)\n",
    "iso2 = countrycodes[\"ISO2 Code\"].str.cat(sep='+')\n",
    "iso3 = countrycodes[\"ISO Code\"].str.cat(sep=',')\n",
    "\n",
    "#Variables\n",
    "variables = \".PCPI_IX+FIGB_PA+BCAXF_BP6_USD+NGDP_R_K_IX+LWR_IX+AIP_SA_IX+FPE_IX.?startPeriod=1976&endPeriod=2018\"\n",
    "\n",
    "url = url+iso2+variables\n",
    "\n",
    "# Get data from the above URL using the requests package\n",
    "data = requests.get(url).json()\n",
    "\n",
    "\n",
    "#### stacking IMF data and creating a dataframe\n",
    "stack = []\n",
    "data2 = pd.DataFrame()\n",
    "for x in range(len(data['CompactData']['DataSet']['Series'])-1):\n",
    "    data2 = pd.DataFrame(data['CompactData']['DataSet']['Series'][x][\"Obs\"])[[\"@OBS_VALUE\", \"@TIME_PERIOD\"]]\n",
    "    data2[\"country\"] = data['CompactData']['DataSet']['Series'][x][\"@REF_AREA\"]\n",
    "    data2[\"indicator\"] = data['CompactData']['DataSet']['Series'][x][\"@INDICATOR\"]\n",
    "    stack.append(data2)\n",
    "stack = pd.concat(stack)\n",
    "\n",
    "\n",
    "stack = stack.set_index([\"@TIME_PERIOD\", 'country', 'indicator']).unstack(level=-1)\n",
    "stack.columns = stack.columns.droplevel(0)\n",
    "\n",
    "stack.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "stack = stack.rename(columns={'@TIME_PERIOD': 'year', \"country\": \"ISO2 Code\"})\n",
    "#Next: Analyze the missing data and data range\n",
    "\n",
    "import missingno as msno\n",
    "msno.matrix(stack)\n",
    "stack = pd.merge(stack, countrycodes[[\"ISO2 Code\", \"ISO Code\"]], on = \"ISO2 Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cif import cif\n",
    "\n",
    "data_all, subjects_all, measures_all = cif.createDataFrameFromOECD(countries = countrycodes[\"ISO Code\"].tolist(), frequency = 'Q', subject = [\"SPASTT01\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd = data_all.stack(level = 0)\n",
    "oecd.columns = oecd.columns.droplevel(0)\n",
    "oecd = oecd.reset_index()\n",
    "oecd = oecd.drop(['GP', 'GY'], axis=1)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.merge(stack, oecd, left_on = [\"ISO Code\", \"year\"], right_on = [\"country\", \"level_0\"], how = \"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.drop([\"AIP_SA_IX\", \"LWR_IX\", \"FPE_IX\", \"ISO Code\", \"level_0\"], axis = 1)\n",
    "msno.matrix(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "z = urlopen('https://www.bis.org/statistics/full_bis_total_credit_csv.zip')\n",
    "myzip = ZipFile(BytesIO(z.read())).extract('WEBSTATS_TOTAL_CREDIT_DATAFLOW_csv_col.csv')\n",
    "credit =pd.read_csv(myzip)\n",
    "\n",
    "\n",
    "credit = credit[(credit[\"Borrowing sector\"] == \"Private non-financial sector\") | (credit[\"Borrowing sector\"] == \"General government\")]\n",
    "credit = credit[(credit[\"Lending sector\"] == \"All sectors\")]\n",
    "credit = credit[(credit[\"BORROWERS_CTY\"].isin(countrycodes[\"ISO2 Code\"]))]\n",
    "credit = credit[(credit[\"Unit type\"] == \"Percentage of GDP\")]\n",
    "credit = credit[(credit[\"TC_ADJUST\"] == \"A\")]\n",
    "credit = credit[(credit[\"Valuation\"] == \"Market value\")]\n",
    "credit = (credit.set_index(['Borrowing sector', 'BORROWERS_CTY'])\n",
    "   .rename_axis(['Year'], axis=1)\n",
    "   .stack()\n",
    "   .unstack('Borrowing sector')\n",
    "   .reset_index())\n",
    "credit = credit.iloc[13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = urlopen('https://www.bis.org/statistics/full_bis_dsr_csv.zip')\n",
    "myzip = ZipFile(BytesIO(z.read())).extract('WEBSTATS_DSR_DATAFLOW_csv_col.csv')\n",
    "debt_service =pd.read_csv(myzip)\n",
    "\n",
    "debt_service = debt_service[(debt_service[\"Borrowers\"] == \"Private non-financial sector\")]\n",
    "debt_service = debt_service[(debt_service[\"BORROWERS_CTY\"].isin(countrycodes[\"ISO2 Code\"]))]\n",
    "debt_service = (debt_service.set_index(['BORROWERS_CTY', \"Borrowers\"])\n",
    "   .rename_axis(['Year'], axis=1)\n",
    "   .stack()\n",
    "   .unstack(\"Borrowers\")\n",
    "   .reset_index())\n",
    "\n",
    "debt_service = debt_service.iloc[5:]\n",
    "debt_service.columns = debt_service.columns = [\"BORROWERS_CTY\", \"Year\", \"debt_service\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_prices = pd.read_csv(\"https://stats.oecd.org/sdmx-json/data/DP_LIVE/.HOUSECOST.../OECD?contentType=csv&detail=code&separator=comma&csv-lang=en\")\n",
    "house_prices = house_prices[(house_prices[\"FREQUENCY\"] == \"Q\")]\n",
    "house_prices = house_prices[(house_prices[\"LOCATION\"].isin(countrycodes[\"ISO Code\"]))]\n",
    "house_prices = house_prices[(house_prices[\"SUBJECT\"] == \"PRICEINCOME\")]\n",
    "house_prices = house_prices.drop(columns=[\"INDICATOR\", \"SUBJECT\", \"MEASURE\", \"FREQUENCY\", \"Flag Codes\"])\n",
    "house_prices.columns = house_prices.columns = [\"LOCATION\", \"TIME\", \"House_prices\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.merge(full, credit, left_on = [\"ISO2 Code\", \"year\"], right_on = [\"BORROWERS_CTY\", \"Year\"], how = \"left\")\n",
    "full = pd.merge(full, debt_service, left_on = [\"ISO2 Code\", \"year\"], right_on = [\"BORROWERS_CTY\", \"Year\"], how = \"left\")\n",
    "full = pd.merge(full, house_prices, left_on = [\"country\", \"year\"], right_on = [\"LOCATION\", \"TIME\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full.drop(columns = [\"country\", \"Year_x\", \"BORROWERS_CTY_x\", \"BORROWERS_CTY_y\", \"Year_y\", \"LOCATION\", \"TIME\"])\n",
    "full[[\"BCAXF_BP6_USD\", \"FIGB_PA\", \"NGDP_R_K_IX\", \"PCPI_IX\", \"IXOB\", \"General government\", \"Private non-financial sector\", \"debt_service\", \"House_prices\"]] = full[[\"BCAXF_BP6_USD\", \"FIGB_PA\", \"NGDP_R_K_IX\", \"PCPI_IX\", \"IXOB\", \"General government\", \"Private non-financial sector\", \"debt_service\", \"House_prices\"]].apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full[\"date\"] = pd.to_datetime(full['year'])\n",
    "full1990 = full[full[\"date\"] > \"1990-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full[\"gdp_growth\"] = full.groupby('ISO2 Code', sort=False).NGDP_R_K_IX.apply(\n",
    "     lambda x: x.pct_change(4))\n",
    "\n",
    "full.rename(columns={'Private non-financial sector':'credit_private', 'General government': 'credit_government'}, inplace=True)\n",
    "\n",
    "full[\"credit_growth\"] = full.groupby('ISO2 Code', sort=False).credit_private.apply(\n",
    "     lambda x: x.pct_change(4))\n",
    "\n",
    "full[\"inflation\"] = full.groupby('ISO2 Code', sort=False).PCPI_IX.apply(\n",
    "     lambda x: x.pct_change(4))\n",
    "\n",
    "full[\"stock_growth\"]  = full.groupby('ISO2 Code', sort=False).IXOB.apply(\n",
    "     lambda x: x.pct_change(4))\n",
    "\n",
    "full[\"house_growth\"]  = full.groupby('ISO2 Code', sort=False).House_prices.apply(\n",
    "     lambda x: x.pct_change(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "### Creating credit gap variable\n",
    "\n",
    "groups = full.groupby('ISO2 Code')\n",
    "\n",
    "group_keys = list(groups.groups.keys())\n",
    "\n",
    "\n",
    "bs = pd.DataFrame()\n",
    "\n",
    "for key in group_keys:\n",
    "\n",
    "    g = groups.get_group(key).copy()\n",
    "    target = g['credit_private']\n",
    "\n",
    "    cycle, trend = sm.tsa.filters.hpfilter(target, lamb=129600)\n",
    "\n",
    "    g['credit_gap'] = trend\n",
    "    bs = bs.append(g)\n",
    "\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating house price gap variable\n",
    "groups = bs.groupby('ISO2 Code')\n",
    "\n",
    "group_keys = list(groups.groups.keys())\n",
    "\n",
    "\n",
    "bs2 = pd.DataFrame()\n",
    "\n",
    "for key in group_keys:\n",
    "\n",
    "    g = groups.get_group(key).copy()\n",
    "    target = g['House_prices']\n",
    "\n",
    "    cycle, trend = sm.tsa.filters.hpfilter(target, lamb=400000)\n",
    "\n",
    "    g['house_gap'] = trend\n",
    "    bs2 = bs2.append(g)\n",
    "\n",
    "bs2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fetching crisis database from ecb paper\n",
    "ecb = pd.read_csv('data/ecbdata.csv', error_bad_lines = False, delimiter = ';')\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb = ecb[ecb['Country'].isin(full['ISO2 Code'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Crisis database taken from the ecb paper\n",
    "ecb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecb.rename(columns={'Start date':'start'}, inplace=True)\n",
    "ecb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### turning to quarters\n",
    "\n",
    "ecb['year'] = pd.PeriodIndex(pd.to_datetime(ecb.start), freq = 'Q').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis = ecb[['Country', 'year']]\n",
    "crisis = crisis.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs2[\"year\"] = bs2[\"year\"].str.replace(\"-\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis[\"dummy\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating crises variable\n",
    "crise = pd.merge(bs2, crisis, left_on = ['ISO2 Code', 'year'], right_on = ['Country', 'year'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crisis.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crise['dummy'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Merged database with quarter of crisis information\n",
    "crise['dummy'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the lags of dependent variable\n",
    "#Backwards\n",
    "crise[\"lag_1\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-1)\n",
    "crise[\"lag_2\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-2)\n",
    "crise[\"lag_3\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-3)\n",
    "crise[\"lag_4\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-4)\n",
    "crise[\"lag_5\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-5)\n",
    "crise[\"lag_6\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-6)\n",
    "crise[\"lag_7\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-7)\n",
    "crise[\"lag_8\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-8)\n",
    "crise[\"lag_9\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-9)\n",
    "crise[\"lag_10\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-10)\n",
    "crise[\"lag_11\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-11)\n",
    "crise[\"lag_12\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(-12)\n",
    "\n",
    "#Front\n",
    "crise[\"lag_1f\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(1)\n",
    "crise[\"lag_2f\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(2)\n",
    "crise[\"lag_3f\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(3)\n",
    "crise[\"lag_4f\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(4)\n",
    "crise[\"lag_5f\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(5)\n",
    "crise[\"lag_6f\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(6)\n",
    "crise[\"lag_7f\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(7)\n",
    "crise[\"lag_8f\"] =crise.groupby(\"ISO2 Code\")[\"dummy\"].shift(8)\n",
    "\n",
    "\n",
    "\n",
    "crise[\"one_year\"] = crise[\"lag_1\"] + crise[\"lag_2\"] + crise[\"lag_3\"] +crise[\"lag_4\"]\n",
    "crise[\"two_year\"] = crise[\"one_year\"] +crise[\"lag_5\"] + crise[\"lag_6\"] + crise[\"lag_7\"] +crise[\"lag_8\"]\n",
    "crise[\"three_year\"] = crise[\"two_year\"] + crise[\"lag_9\"] + crise[\"lag_10\"] + crise[\"lag_11\"] +crise[\"lag_12\"]\n",
    "\n",
    "crise[\"one_year_f\"] = crise[\"lag_1f\"] + crise[\"lag_2f\"] + crise[\"lag_3f\"] +crise[\"lag_4f\"]\n",
    "crise[\"two_year_f\"] = crise[\"one_year_f\"] +crise[\"lag_5f\"] + crise[\"lag_6f\"] + crise[\"lag_7f\"] +crise[\"lag_8f\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing \n",
    "\n",
    "#crise = crise[crise['dummy'] == 0]\n",
    "#crise = crise[crise[\"two_year_f\"] == 0]\n",
    "#crise = crise[crise[\"one_year\"] == 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crise.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crise.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing machine learning pipeline \n",
    "\n",
    "from pandas import set_option\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crise = crise[crise[\"date\"] < \"2018-01-01\"]\n",
    "X =  crise[['ISO2 Code','BCAXF_BP6_USD', 'FIGB_PA', 'credit_government', 'credit_private', 'debt_service', 'House_prices', 'gdp_growth', 'credit_growth', 'inflation', 'stock_growth', 'credit_gap', 'house_gap', \"house_growth\"]]\n",
    "Y = crise['three_year']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filling nas in database.\n",
    "\n",
    "X.groupby(\"ISO2 Code\").ffill()\n",
    "Y.fillna(0, inplace = True)\n",
    "X.fillna(X.median(), inplace = True)\n",
    "\n",
    "X = pd.get_dummies(X, columns=['ISO2 Code'], prefix = ['Country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,Y,\n",
    "                                                   test_size=0.25,\n",
    "                                                   random_state=0,\n",
    "                                                   stratify=crise['three_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot-Check Algorithms\n",
    "def GetBasedModel():\n",
    "    basedModels = []\n",
    "    basedModels.append(('LR'   , LogisticRegression()))\n",
    "    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))\n",
    "    basedModels.append(('KNN'  , KNeighborsClassifier()))\n",
    "    basedModels.append(('CART' , DecisionTreeClassifier()))\n",
    "    basedModels.append(('NB'   , GaussianNB()))\n",
    "    basedModels.append(('SVM'  , SVC(probability=True)))\n",
    "    basedModels.append(('AB'   , AdaBoostClassifier()))\n",
    "    basedModels.append(('GBM'  , GradientBoostingClassifier()))\n",
    "    basedModels.append(('RF'   , RandomForestClassifier()))\n",
    "    basedModels.append(('ET'   , ExtraTreesClassifier()))\n",
    "\n",
    "    \n",
    "    return basedModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def BasedLine2(X_train, y_train,models):\n",
    "    # Test options and evaluation metric\n",
    "    num_folds = 10\n",
    "    scoring = 'roc_auc'\n",
    "\n",
    "    results = []\n",
    "    names = []\n",
    "    for name, model in models:\n",
    "        kfold = StratifiedKFold(n_splits=num_folds, random_state= 1990)\n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "        \n",
    "    return names, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import graph_objs as go\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PlotBoxR(object):\n",
    "    \n",
    "    \n",
    "    def __Trace(self,nameOfFeature,value): \n",
    "    \n",
    "        trace = go.Box(\n",
    "            y=value,\n",
    "            name = nameOfFeature,\n",
    "            marker = dict(\n",
    "                color = 'rgb(0, 128, 128)',\n",
    "            )\n",
    "        )\n",
    "        return trace\n",
    "\n",
    "    def PlotResult(self,names,results):\n",
    "        \n",
    "        data = []\n",
    "\n",
    "        for i in range(len(names)):\n",
    "            data.append(self.__Trace(names[i],results[i]))\n",
    "\n",
    "\n",
    "        py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GetBasedModel()\n",
    "names,results = BasedLine2(X_train, y_train,models)\n",
    "PlotBoxR().PlotResult(names,results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_2 = preprocessing.scale(X)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test =train_test_split(X_2,Y,\n",
    "                                                   test_size=0.3,\n",
    "                                                   random_state=0,\n",
    "                                                   stratify=Y)\n",
    "\n",
    "model = ExtraTreesClassifier(n_jobs=2, min_samples_split = 2)\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid={\n",
    "        'n_estimators': range(1600, 1601, 1),\n",
    "        \n",
    "    },\n",
    "    scoring='roc_auc',\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "grid_result = gsc.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "for test_mean, train_mean, param in zip(\n",
    "        grid_result.cv_results_['mean_test_score'],\n",
    "        grid_result.cv_results_['mean_train_score'],\n",
    "        grid_result.cv_results_['params']):\n",
    "    print(\"Train: %f // Test : %f with: %r\" % (train_mean, test_mean, param))\n",
    "    \n",
    "model = ExtraTreesClassifier(**grid_result.best_params_)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = model.predict_proba(X_2[crise[\"ISO2 Code\"] == \"PT\"])[:,1]\n",
    "\n",
    "\n",
    "\n",
    "pt = crise[crise[\"ISO2 Code\"] == \"PT\"]\n",
    "pt[\"a\"] = a\n",
    "pt = pt.set_index('year')\n",
    "pt = pd.merge(crise, pt, on = ['ISO2 Code', 'year'], how = \"left\")\n",
    "pt = pt[pt[\"ISO2 Code\"] == \"PT\"]\n",
    "pt[\"year\"] = pd.to_datetime(pt['year'])\n",
    "pt = pt.set_index('year')\n",
    "ax = pt['a'].plot(linewidth=0.5)\n",
    "ax.axvline(\"2010-01-01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "a = model.predict_proba(X_2)[:,1]\n",
    "\n",
    "\n",
    "\n",
    "pt = crise\n",
    "pt[\"a\"] = a\n",
    "pt = pt.set_index('year')\n",
    "pt = pd.merge(crise, pt, on = ['ISO2 Code', 'year'], how = \"left\")\n",
    "pt[\"year\"] = pd.to_datetime(pt['year'])\n",
    "pt = pt.set_index('year')\n",
    "g = sns.FacetGrid(pt, col=\"ISO2 Code\")\n",
    "g = g.map(plt.plot, \"year\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ScoreDataFrame(names,results):\n",
    "    def floatingDecimals(f_val, dec=3):\n",
    "        prc = \"{:.\"+str(dec)+\"f}\" \n",
    "    \n",
    "        return float(prc.format(f_val))\n",
    "\n",
    "    scores = []\n",
    "    for r in results:\n",
    "        scores.append(floatingDecimals(r.mean(),4))\n",
    "\n",
    "    scoreDataFrame = pd.DataFrame({'Model':names, 'Score': scores})\n",
    "    return scoreDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedLineScore = ScoreDataFrame(names,results)\n",
    "basedLineScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def GetScaledModel(nameOfScaler):\n",
    "    \n",
    "    if nameOfScaler == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    elif nameOfScaler =='minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "    pipelines = []\n",
    "    pipelines.append((nameOfScaler+'LR'  , Pipeline([('Scaler', scaler),('LR'  , LogisticRegression())])))\n",
    "    pipelines.append((nameOfScaler+'LDA' , Pipeline([('Scaler', scaler),('LDA' , LinearDiscriminantAnalysis())])))\n",
    "    pipelines.append((nameOfScaler+'KNN' , Pipeline([('Scaler', scaler),('KNN' , KNeighborsClassifier())])))\n",
    "    pipelines.append((nameOfScaler+'CART', Pipeline([('Scaler', scaler),('CART', DecisionTreeClassifier())])))\n",
    "    pipelines.append((nameOfScaler+'NB'  , Pipeline([('Scaler', scaler),('NB'  , GaussianNB())])))\n",
    "    pipelines.append((nameOfScaler+'SVM' , Pipeline([('Scaler', scaler),('SVM' , SVC())])))\n",
    "    pipelines.append((nameOfScaler+'AB'  , Pipeline([('Scaler', scaler),('AB'  , AdaBoostClassifier())])  ))\n",
    "    pipelines.append((nameOfScaler+'GBM' , Pipeline([('Scaler', scaler),('GMB' , GradientBoostingClassifier())])  ))\n",
    "    pipelines.append((nameOfScaler+'RF'  , Pipeline([('Scaler', scaler),('RF'  , RandomForestClassifier())])  ))\n",
    "    pipelines.append((nameOfScaler+'ET'  , Pipeline([('Scaler', scaler),('ET'  , ExtraTreesClassifier())])  ))\n",
    "\n",
    "\n",
    "    return pipelines \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GetScaledModel('standard')\n",
    "names,results = BasedLine2(X_train, y_train,models)\n",
    "PlotBoxR().PlotResult(names,results)\n",
    "scaledScoreStandard = ScoreDataFrame(names,results)\n",
    "compareModels = pd.concat([basedLineScore,\n",
    "                           scaledScoreStandard], axis=1)\n",
    "compareModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=1990)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = GetScaledModel('minmax')\n",
    "names,results = BasedLine2(X_train, y_train,models)\n",
    "PlotBoxR().PlotResult(names,results)\n",
    "\n",
    "scaledScoreMinMax = ScoreDataFrame(names,results)\n",
    "compareModels = pd.concat([basedLineScore,\n",
    "                           scaledScoreStandard,\n",
    "                          scaledScoreMinMax], axis=1)\n",
    "compareModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feature importance\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=1990)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot feature importance\n",
    "feature_importance = clf.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])#boston.feature_names[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model interpretability via shapley values\n",
    "\n",
    "import shap\n",
    "\n",
    "shap_values = shap.TreeExplainer(model).shap_values(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting show=False allows us to continue customizing the matplotlib plot before displaying it\n",
    "shap.dependence_plot(\"inflation\", shap_values, X_train, show=False)\n",
    "pl.xlim(80,225)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
